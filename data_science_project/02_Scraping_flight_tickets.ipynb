{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling and web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, they have APIs but they have no well-written packages in the language you prefer (e.g. only Java but no Python libraries). Even worse, there may not be APIs for the public and we have to design a scraper to retrieve all the relevant informaiton we want. In such cases, we can manually build our own wrapper functions.\n",
    "\n",
    "Web crawling and web scraping are two related techniques used to extract information from websites.\n",
    "\n",
    "Web crawling, also known as web indexing or web spidering, is the process of automatically exploring and indexing web pages on the internet. Web crawlers, also called spiders, bots, or robots, navigate through websites, follow links, and index the content of the pages they encounter. Search engines like Google and Bing use web crawlers to build their indexes of web pages, which enables users to find information easily.\n",
    "\n",
    "Web scraping, on the other hand, is the process of extracting specific data from web pages. Web scraping involves analyzing the HTML structure of a webpage, identifying the relevant information, and extracting it into a structured format such as a CSV or JSON file. Web scraping can be used to extract product information, pricing data, news articles, and more.\n",
    "\n",
    "Web crawling and web scraping can be done manually, but it's often more efficient to use specialized software tools. Python is a popular language for web crawling and web scraping, and there are many libraries available, including BeautifulSoup, Scrapy, and Selenium.\n",
    "\n",
    "However, it's important to note that web scraping can raise legal and ethical concerns, particularly if done without permission or in violation of website terms of service. Web scraping can also put a strain on website servers, potentially causing them to crash or become unavailable. As such, it's important to use web scraping responsibly and within legal and ethical boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminiary examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from <a href=\"https://www.w3schools.com/html/tryit.asp?filename=tryhtml_basic_document\" target=\"blank_\">w3schools</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1>My First Heading</h1>\n",
    "\n",
    "<p>My 1st paragraph.</p>\n",
    "<p>My 2nd paragraph.</p>\n",
    "<p>My 3rd paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this code to your disk as `sample.html` (or any other name). We will use a great library called ___`Beautiful Soup`___ to read the contents from Python. You may also need to install lxml, which is for parsing specific formats (e.g., html and xml).\n",
    "\n",
    "    pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T00:59:26.608656Z",
     "start_time": "2019-03-15T00:59:26.448596Z"
    }
   },
   "outputs": [],
   "source": [
    "## Do the following if you have not\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "to_location = 'AGP'\n",
    "url = 'https://www.kayak.dk/flights/BLL-{to_location}/2024-08-03-flexible-2days/2adults?fs=cfc=1&sort=bestflight_a'.format(to_location=to_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "to_location = 'AGP'\n",
    "from_location = 'BLL'\n",
    "date_start = '2024-08-03'\n",
    "\n",
    "url = 'https://www.kayak.dk/flights/{from_location}-{to_location}/{date_start}-flexible-2days/2adults?fs=cfc=1&sort=bestflight_a'.format(to_location=to_location, from_location=from_location, date_start=date_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "# Attempt to click on the popup window\n",
    "sleep(10)\n",
    "driver.find_element(\"xpath\", '//*[@id=\"portal-container\"]/div/div[2]/div/div/div[1]/div/span[2]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_more():\n",
    "    try:\n",
    "        driver.find_element(\"xpath\", '//*[@id=\"c4l5n\"]/div/div').click()\n",
    "        print('sleeping.....')\n",
    "        sleep(randint(25,35))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_rows = driver.find_elements(By.XPATH, '//div[@class=\"nrc6-wrapper\"]')\n",
    "print(flight_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_prices =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for WebElement in flight_rows:\n",
    "    elementHTML = WebElement.get_attribute('outerHTML')\n",
    "    elementSoup = Soup(elementHTML, 'lxml')\n",
    "\n",
    "    # Now continue can be used properly within the loop\n",
    "    try:\n",
    "        # Your parsing code goes here\n",
    "        # For example:\n",
    "        temp_price = elementSoup.find(\"div\", {\"class\": 'nrc6-price-section'})\n",
    "        temp_airline = elementSoup.find(\"div\", {\"class\": 'nrc6-default-footer'})\n",
    "        temp_duration = elementSoup.find(\"div\", {\"class\": 'nrc6-main'})\n",
    "        #temp_stop = elementSoup.find(\"div\", {\"class\": 'hJSA'})\n",
    "        \n",
    "        price = temp_price.find(\"div\", {\"class\": \"f8F1-price-text\"})\n",
    "        airline = temp_airline.find(\"div\", {\"class\": \"J0g6-operator-text\"})\n",
    "        duration = temp_duration.find(\"div\", {\"class\": \"xdW8 xdW8-mod-full-airport\"})\n",
    "        stop = temp_duration.find(\"span\", {\"class\": \"JWEO-stops-text\"})\n",
    "        time = temp_duration.find(\"div\", {\"class\": \"vmXl vmXl-mod-variant-large\"})\n",
    "        where_stop = temp_duration.find(\"div\", {\"class\": \"\"})\n",
    "        \n",
    "        lst_prices.append(price.text)\n",
    "        lst_prices.append(airline.text)\n",
    "        lst_prices.append(duration.text)\n",
    "        lst_prices.append(stop.text)\n",
    "        lst_prices.append(time.text)\n",
    "        pass\n",
    "    except AttributeError:\n",
    "        print(\"Attribute error occurred, skipping this element.\")\n",
    "        continue  # Move to the next iteration of the loop if there's an attribute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'capabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-ZIYGBZBM-py3.11\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mSeleniumManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-ZIYGBZBM-py3.11\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:87\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determines the path of the correct driver.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m:Args:\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m - browser: which browser to get the driver path for.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m:Returns: The driver path to use\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m browser \u001b[38;5;241m=\u001b[39m \u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapabilities\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     89\u001b[0m args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_binary()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--browser\u001b[39m\u001b[38;5;124m\"\u001b[39m, browser]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mChristian\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mchromedriver_win32\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mchromedriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m sleep(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-ZIYGBZBM-py3.11\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-ZIYGBZBM-py3.11\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:49\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new WebDriver instance of the ChromiumDriver. Starts the\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mservice and then creates new WebDriver instance of ChromiumDriver.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m - keep_alive - Whether to configure ChromiumRemoteConnection to use HTTP keep-alive.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[43mDriverFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\data-science-project-ZIYGBZBM-py3.11\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:40\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     38\u001b[0m     path \u001b[38;5;241m=\u001b[39m SeleniumManager()\u001b[38;5;241m.\u001b[39mdriver_location(options) \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m---> 40\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapabilities\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Selenium Manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\Christian\\Downloads\\chromedriver_win32\\chromedriver\")\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_kayak(to_location, from_location, date_start):\n",
    "    \"\"\"City codes - it's the IATA codes!\n",
    "    Date format -  YYYY-MM-DD\"\"\"\n",
    "    \n",
    "    #to_location = 'AGP'\n",
    "    #from_location = 'BLL'\n",
    "    #date_start = '2024-08-03'\n",
    "    url = 'https://www.kayak.dk/flights/{from_location}-{to_location}/{date_start}-flexible-2days/2adults?fs=cfc=1&sort=bestflight_a'.format(to_location=to_location, from_location=from_location, date_start=date_start)\n",
    "    driver.get(url)\n",
    "    sleep(10)\n",
    "    \n",
    "#Click on the popup window:\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"portal-container\"]/div/div[2]/div/div/div[1]/div/span[2]/button').click()\n",
    "    print('Popup closed.....')\n",
    "\n",
    "#Click on the 'load more' button in the end of the page:\n",
    "    driver.find_element(\"xpath\", '//*[@id=\"c4l5n\"]/div/div').click()\n",
    "    print('Loading more...')\n",
    "\n",
    "    #Start the first scrape\n",
    "    print('starting first scrape.....')\n",
    "    flight_rows = driver.find_elements(By.XPATH, '//div[@class=\"nrc6-wrapper\"]')\n",
    "    #Create empty list\n",
    "    lst_prices =[]\n",
    "\n",
    "#Scrape through the WebElements from flight rows and extract desired data\n",
    "    for WebElement in flight_rows:\n",
    "        elementHTML = WebElement.get_attribute('outerHTML')\n",
    "        elementSoup = Soup(elementHTML, 'lxml')\n",
    "\n",
    "        try:\n",
    "            # Your parsing code goes here\n",
    "            # For example:\n",
    "            temp_price = elementSoup.find(\"div\", {\"class\": 'nrc6-price-section'})\n",
    "            temp_airline = elementSoup.find(\"div\", {\"class\": 'nrc6-default-footer'})\n",
    "            temp_duration = elementSoup.find(\"div\", {\"class\": 'nrc6-main'})\n",
    "            #temp_stop = elementSoup.find(\"div\", {\"class\": 'hJSA'})\n",
    "        \n",
    "            price = temp_price.find(\"div\", {\"class\": \"f8F1-price-text\"})\n",
    "            airline = temp_airline.find(\"div\", {\"class\": \"J0g6-operator-text\"})\n",
    "            duration = temp_duration.find(\"div\", {\"class\": \"xdW8 xdW8-mod-full-airport\"})\n",
    "            stop = temp_duration.find(\"span\", {\"class\": \"JWEO-stops-text\"})\n",
    "            time = temp_duration.find(\"div\", {\"class\": \"vmXl vmXl-mod-variant-large\"})\n",
    "            where_stop = temp_duration.find(\"div\", {\"class\": \"\"})\n",
    "        \n",
    "            lst_prices.append(price.text)\n",
    "            lst_prices.append(airline.text)\n",
    "            lst_prices.append(duration.text)\n",
    "            lst_prices.append(stop.text)\n",
    "            lst_prices.append(time.text)\n",
    "        except AttributeError:\n",
    "            print(\"Attribute error occurred, skipping this element.\")\n",
    "            continue  # Move to the next iteration of the loop if there's an attribute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_location = 'AGP'\n",
    "from_location = 'BLL'\n",
    "date_start = '2024-08-03'\n",
    "\n",
    "start_kayak(to_location, from_location, date_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, you can scrape almost any webpage of interest. Other formats such as <a href=\"http://www.json.org/\" target=\"_blank\">JSON</a> and <a href=\"https://www.w3.org/XML/\" target=\"_blank\">XML</a> do have high similarities and a few differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***But keep in mind that you should act politely, with propoer permission!! To find out whether specific paths/contents are allowed to be scraped, you can check their ___`robots.txt`___. For example, <a href=\"https://www.google.com/robots.txt\" target=\"_blank\">here's</a> the permission information set by Google.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the examples we are using here are relatively simple. There are cases that we cannot access the pagination/scoll simply by `requests` alone. In those cases, [Selenium](http://selenium-python.readthedocs.io/) will save our lifes by ___simulating Browsers___!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more tutorials/tools:\n",
    "\n",
    "- https://scrapy.org/ #building a crawler \n",
    "- https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "- https://www.quora.com/Python-programming-language-1/How-is-BeautifulSoup-different-from-Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "return to [overview](../00_overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "133px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
